{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Larq Zoo Tutorial\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/larq/docs/blob/master/docs/zoo/tutorials.ipynb\"><button class=\"notebook-badge\">Run on Colab</button></a> <a href=\"https://github.com/larq/docs/blob/master/docs/zoo/tutorials.ipynb\"><button class=\"notebook-badge\">View on GitHub</button></a>\n",
    "\n",
    "This tutorial demonstrates how to load pretrained models from Larq Zoo. These models can be used for prediction, feature extraction, and fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install larq larq-zoo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import larq_zoo as lqz\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and prepare a sample image\n",
    "\n",
    "In the following we will use a sample image from the [ImageNet](http://image-net.org/) dataset:\n",
    "<img src=\"https://raw.githubusercontent.com/larq/zoo/master/tests/fixtures/elephant.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "img_path = \"https://raw.githubusercontent.com/larq/zoo/master/tests/fixtures/elephant.jpg\"\n",
    "\n",
    "with urlopen(img_path) as f:\n",
    "    img = Image.open(f).resize((224, 224))\n",
    "\n",
    "x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "x = lqz.preprocess_input(x)\n",
    "x = np.expand_dims(x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify ImageNet classes with QuickNet\n",
    "\n",
    "We will first load the QuickNet architecture with pretrained weights and predict the image class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n02504458', 'African_elephant', 0.57994586),\n",
       " ('n01871265', 'tusker', 0.41163084),\n",
       " ('n02504013', 'Indian_elephant', 0.008422509),\n",
       " ('n02410509', 'bison', 1.6847167e-07),\n",
       " ('n02412080', 'ram', 1.3900193e-07)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lqz.sota.QuickNet(weights=\"imagenet\")\n",
    "preds = model.predict(x)\n",
    "lqz.decode_predictions(preds, top=5)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features with QuickNet\n",
    "\n",
    "Larq Zoo models can also be used to extract features that can be used as input to a second model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (1, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = lqz.sota.QuickNet(weights=\"imagenet\", include_top=False)\n",
    "features = model.predict(x)\n",
    "print(\"Feature shape:\", features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features from an arbitrary intermediate layer\n",
    "\n",
    "Features can also be extracted from arbitrary intermediate layer with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_7 feature shape: (1, 14, 14, 256)\n"
     ]
    }
   ],
   "source": [
    "avg_pool_layer = model.get_layer(\"add_7\")\n",
    "avg_pool_model = tf.keras.models.Model(\n",
    "    inputs=model.input, outputs=avg_pool_layer.output)\n",
    "\n",
    "avg_pool_features = avg_pool_model.predict(x)\n",
    "print(\"add_7 feature shape:\", avg_pool_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build QuickNet over a custom input Tensor\n",
    "\n",
    "The model can also be used with an input Tensor that might also be the output a different Keras model or layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "input_tensor = tf.keras.layers.Input(shape=(224, 224, 3))\n",
    "\n",
    "model = lqz.sota.QuickNet(input_tensor=input_tensor, weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate QuickNet with TensorFlow Datasets\n",
    "\n",
    "To re-run the evaluation on the entire [ImageNet](http://image-net.org/) validation dataset [Tensorflow Datasets](https://www.tensorflow.org/datasets/) can be used.\n",
    "\n",
    "Note that running this example will require [**mannualy downloading**](https://www.tensorflow.org/datasets/catalog/imagenet2012) the entire dataset and might take a very long time to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    img = lqz.preprocess_input(data[\"image\"])\n",
    "    label = tf.one_hot(data[\"label\"], 1000)\n",
    "    return img, label \n",
    "\n",
    "dataset = (\n",
    "    tfds.load(\"imagenet2012:5.0.0\", split=tfds.Split.VALIDATION)\n",
    "    .map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .batch(128)\n",
    "    .prefetch(1)\n",
    ")\n",
    "\n",
    "model = lqz.sota.QuickNet()\n",
    "model.compile(\n",
    "    optimizer=\"sgd\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"categorical_accuracy\", \"top_k_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "model.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

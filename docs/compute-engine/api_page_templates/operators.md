# Larq Compute Engine Operators

Larq Compute Engine extends the [builtin TensorFlow Lite operators](https://www.tensorflow.org/lite/guide/ops_compatibility#tensorflow_lite_operations) with optimized operators for running binarized neural networks.

## Data Format

Larq Compute Engine adheres to the `NHWC` data format used in TensorFlow Lite for activation tensors. In the following a tensor of format `BitTensor<int32, n>` represents a TensorFlow Lite tensor with data type `int32` containing binary values which are bitpacked across the channel dimension `n` and potentially padded with zeros. Mathematically a `0` valued bit represents a real value of `1.0` while `1` is interpreted as `-1.0`.

Options for the operators are stored using [FlexBuffers](https://google.github.io/flatbuffers/flexbuffers.html) in the `.tflite` model files generated by the [Converter](/compute-engine/api/converter/) and passed as opaque values to the custom operators.

## List of Operations

### LceBconv2d

2D binarized convolution layer.

The operation will compute

\\[
y_{n,\mathrm{out}} = \beta_\mathrm{out} + \gamma_\mathrm{out} \, \sigma\left(\sum_{i = 0}^{I - 1} w_{\mathrm{out}, i} \star \mathrm{bsign}(x_{n,i})\right), \quad n \in [0, N), \, \mathrm{out} \in [0, O)
\\]

where \\(\star\\) is the 2D cross-correlation operator, \\(N\\) is a batch size, \\(I\\) and \\(O\\) denote the number of input and output channels, and \\(\mathrm{bsign}\\) is the [binary sign function](/larq/api/math/#sign-function).

**Inputs**

- `Tensor<float32|int8> | BitTensor<int32, 3>`: 4D input tensor \\(x\\)
- `BitTensor<int32, 3>`: 4D bitpacked binary filter tensor \\(w\\) in `OHWI` format
- `Tensor<float32>`: 1D post activation multiplier \\(\gamma\\)
- `Tensor<float32>`: 1D post activation bias \\(\beta\\)

**Outputs**

- `Tensor<float32|int8> | BitTensor<int32, 3>`: Result of the 2D convolution of the input tensor \\(y\\)

**Options**

- **channels_in** `int32`: Number of input channels of the incoming activations. This is necessary since input channels cannot be inferred from the shape of weights and activations if both are bitpacked.
- **dilation_height_factor** `int32`: Vertical dilation rate of the filter window
- **dilation_width_factor** `int32`: Horizontal dilation rate of the filter window
- **fused_activation_function** `string`: \\(\sigma\\), one of `"NONE"`, `"RELU"`, `"RELU_N1_TO_1"` or `"RELU6"`
- **padding** `string`: One of `"SAME"` or `"VALID"`
- **stride_height** `int32`: Vertical stride of the filter window
- **stride_width** `int32`: Horizontal stride of the filter window

---

### LceBMaxPool2d

Max pooling operation on spatial input data.

**Inputs**

- `Tensor<float32|int8> | BitTensor<int32, 3>`: 4D input tensor

**Outputs**

- `BitTensor<int32, 3>`: A tensor where each entry is the maximum of the input values in the corresponding window.

**Options**

- **padding** `string`: One of `"SAME"` or `"VALID"`
- **stride_width** `int32`: Horizontal stride of the sliding window
- **stride_height** `int32`: Vertical stride of the sliding window
- **filter_width** `int32`: Horizontal size of the sliding window
- **filter_height** `int32`: Vertical size of the sliding window

{{autogenerated}}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End Guide: Converting and Benchmarking a Model\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/larq/docs/blob/master/docs/compute-engine/end_to_end.ipynb\"><button class=\"notebook-badge\">Run on Colab</button></a> <a href=\"https://github.com/larq/docs/blob/master/docs/compute-engine/end_to_end.ipynb\"><button class=\"notebook-badge\">View on GitHub</button></a>\n",
    "\n",
    "These steps will walk you through deploying a BNN with LCE. The guide starts by downloading, converting and benchmarking a model from [Larq Zoo](/zoo/), and will then discuss the process for a custom model.\n",
    "\n",
    "## Picking a model from Larq Zoo\n",
    "\n",
    "This example uses the [QuickNet model](/zoo/api/sota/#quicknet) from the `sota` submodule of `larq-zoo`.\n",
    "First, install the Larq Ecosystem pip packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install larq larq-zoo larq-compute-engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create a python script that will download QuickNet and print the model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+quicknet stats----------------------------------------------------------------------------------------------------+\n",
      "| Layer                   Input prec.                 Outputs  # 1-bit  # 32-bit   Memory  1-bit MACs  32-bit MACs |\n",
      "|                               (bit)                              x 1       x 1     (kB)                          |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| input_1                           -  ((None, 224, 224, 3),)        0         0        0           ?            ? |\n",
      "| conv2d                            -       (-1, 112, 112, 8)        0       216     0.84           0      2709504 |\n",
      "| depthwise_conv2d                  -        (-1, 56, 56, 64)        0       576     2.25           0      1806336 |\n",
      "| batch_normalization               -        (-1, 56, 56, 64)        0       128     0.50           0            0 |\n",
      "| quant_conv2d                      1        (-1, 56, 56, 64)    36864         0     4.50   115605504            0 |\n",
      "| batch_normalization_1             -        (-1, 56, 56, 64)        0       128     0.50           0            0 |\n",
      "| add                               -        (-1, 56, 56, 64)        0         0        0           ?            ? |\n",
      "| quant_conv2d_1                    1        (-1, 56, 56, 64)    36864         0     4.50   115605504            0 |\n",
      "| batch_normalization_2             -        (-1, 56, 56, 64)        0       128     0.50           0            0 |\n",
      "| add_1                             -        (-1, 56, 56, 64)        0         0        0           ?            ? |\n",
      "| max_pooling2d                     -        (-1, 28, 28, 64)        0         0        0           0            0 |\n",
      "| quant_conv2d_2                    1        (-1, 28, 28, 64)    36864         0     4.50    28901376            0 |\n",
      "| batch_normalization_3             -        (-1, 28, 28, 64)        0       128     0.50           0            0 |\n",
      "| batch_normalization_4             -        (-1, 28, 28, 64)        0       128     0.50           0            0 |\n",
      "| add_2                             -        (-1, 28, 28, 64)        0         0        0           ?            ? |\n",
      "| concatenate                       -       (-1, 28, 28, 128)        0         0        0           ?            ? |\n",
      "| quant_conv2d_3                    1       (-1, 28, 28, 128)   147456         0    18.00   115605504            0 |\n",
      "| batch_normalization_5             -       (-1, 28, 28, 128)        0       256     1.00           0            0 |\n",
      "| add_3                             -       (-1, 28, 28, 128)        0         0        0           ?            ? |\n",
      "| quant_conv2d_4                    1       (-1, 28, 28, 128)   147456         0    18.00   115605504            0 |\n",
      "| batch_normalization_6             -       (-1, 28, 28, 128)        0       256     1.00           0            0 |\n",
      "| add_4                             -       (-1, 28, 28, 128)        0         0        0           ?            ? |\n",
      "| max_pooling2d_1                   -       (-1, 14, 14, 128)        0         0        0           0            0 |\n",
      "| quant_conv2d_5                    1       (-1, 14, 14, 128)   147456         0    18.00    28901376            0 |\n",
      "| batch_normalization_7             -       (-1, 14, 14, 128)        0       256     1.00           0            0 |\n",
      "| batch_normalization_8             -       (-1, 14, 14, 128)        0       256     1.00           0            0 |\n",
      "| add_5                             -       (-1, 14, 14, 128)        0         0        0           ?            ? |\n",
      "| concatenate_1                     -       (-1, 14, 14, 256)        0         0        0           ?            ? |\n",
      "| quant_conv2d_6                    1       (-1, 14, 14, 256)   589824         0    72.00   115605504            0 |\n",
      "| batch_normalization_9             -       (-1, 14, 14, 256)        0       512     2.00           0            0 |\n",
      "| add_6                             -       (-1, 14, 14, 256)        0         0        0           ?            ? |\n",
      "| quant_conv2d_7                    1       (-1, 14, 14, 256)   589824         0    72.00   115605504            0 |\n",
      "| batch_normalization_10            -       (-1, 14, 14, 256)        0       512     2.00           0            0 |\n",
      "| add_7                             -       (-1, 14, 14, 256)        0         0        0           ?            ? |\n",
      "| quant_conv2d_8                    1       (-1, 14, 14, 256)   589824         0    72.00   115605504            0 |\n",
      "| batch_normalization_11            -       (-1, 14, 14, 256)        0       512     2.00           0            0 |\n",
      "| add_8                             -       (-1, 14, 14, 256)        0         0        0           ?            ? |\n",
      "| max_pooling2d_2                   -         (-1, 7, 7, 256)        0         0        0           0            0 |\n",
      "| quant_conv2d_9                    1         (-1, 7, 7, 256)   589824         0    72.00    28901376            0 |\n",
      "| batch_normalization_12            -         (-1, 7, 7, 256)        0       512     2.00           0            0 |\n",
      "| batch_normalization_13            -         (-1, 7, 7, 256)        0       512     2.00           0            0 |\n",
      "| add_9                             -         (-1, 7, 7, 256)        0         0        0           ?            ? |\n",
      "| concatenate_2                     -         (-1, 7, 7, 512)        0         0        0           ?            ? |\n",
      "| quant_conv2d_10                   1         (-1, 7, 7, 512)  2359296         0   288.00   115605504            0 |\n",
      "| batch_normalization_14            -         (-1, 7, 7, 512)        0      1024     4.00           0            0 |\n",
      "| add_10                            -         (-1, 7, 7, 512)        0         0        0           ?            ? |\n",
      "| quant_conv2d_11                   1         (-1, 7, 7, 512)  2359296         0   288.00   115605504            0 |\n",
      "| batch_normalization_15            -         (-1, 7, 7, 512)        0      1024     4.00           0            0 |\n",
      "| add_11                            -         (-1, 7, 7, 512)        0         0        0           ?            ? |\n",
      "| quant_conv2d_12                   1         (-1, 7, 7, 512)  2359296         0   288.00   115605504            0 |\n",
      "| batch_normalization_16            -         (-1, 7, 7, 512)        0      1024     4.00           0            0 |\n",
      "| add_12                            -         (-1, 7, 7, 512)        0         0        0           ?            ? |\n",
      "| activation                        -         (-1, 7, 7, 512)        0         0        0           ?            ? |\n",
      "| average_pooling2d                 -         (-1, 1, 1, 512)        0         0        0           0            0 |\n",
      "| flatten                           -               (-1, 512)        0         0        0           0            0 |\n",
      "| dense                             -              (-1, 1000)        0    513000  2003.91           0       512000 |\n",
      "| activation_1                      -              (-1, 1000)        0         0        0           ?            ? |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Total                                                        9990144    521088  3255.00  1242759168      5027840 |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "+quicknet summary-----------------------------+\n",
      "| Total params                      10.5 M    |\n",
      "| Trainable params                  10.5 M    |\n",
      "| Non-trainable params              7.3 k     |\n",
      "| Model size                        3.18 MiB  |\n",
      "| Model size (8-bit FP weights)     1.69 MiB  |\n",
      "| Float-32 Equivalent               40.10 MiB |\n",
      "| Compression Ratio of Memory       0.08      |\n",
      "| Number of MACs                    1.25 B    |\n",
      "| Ratio of MACs that are binarized  0.9960    |\n",
      "+---------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import larq as lq\n",
    "import larq_compute_engine as lce\n",
    "import larq_zoo as lqz\n",
    "\n",
    "\n",
    "# Load the QuickNet architecture and download the weights for ImageNet\n",
    "model = lqz.sota.QuickNet(weights=\"imagenet\")\n",
    "lq.models.summary(model)\n",
    "model.save(\"quicknet.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model size is 3.18 MiB, but the float-32 model size is 40.10 MiB. Indeed, if you look at the `quicknet.h5` file you just saved, you'll see that it is around 42 MiB in size. This is because the model is currently unoptimized and the weights are still stored as floats rather than binary values, so executing this model on any device wouldn't be very fast at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the model\n",
    "\n",
    "Larq Compute Engine is built on top of TensorFlow Lite, and therefore uses the [TensorFlow Lite FlatBuffer format](https://google.github.io/flatbuffers/) to convert and serialize Larq models for inference. We provide our own [LCE Model Converter](/compute-engine/api/converter/) to convert models from Keras to flatbuffers, containing additional optimization passes that increase the execution speed of Larq models on the supported target platforms. \n",
    "\n",
    "Using this converter is very simple, and can be done by adding the following code to the python script above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our Keras model to a TFLite flatbuffer file\n",
    "with open(\"quicknet.tflite\", \"wb\") as flatbuffer_file:\n",
    "    flatbuffer_bytes = lce.convert_keras_model(model)\n",
    "    flatbuffer_file.write(flatbuffer_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce the converted `quicknet.tflite` file with compressed weights and optimized operations, which is only just over 3 MiB in size!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "This part of the guide assumes that you'll want to benchmark on an 64-bit ARM based system such as a Raspberry Pi. For more detailed instructions on benchmarking and for benchmarking on Android phones, see the [Benchmarking guide](/compute-engine/benchmark).\n",
    "\n",
    "On ARM, benchmarking is as simple as downloading the pre-built benchmarking binary from the [latest release](https://github.com/larq/compute-engine/releases/latest) to the target device and running it with the converted model:\n",
    "\n",
    "!!! warning\n",
    "        The following code should be executed on the target platform, e.g. a Raspberry Pi. The exclamation marks should be          removed, but are necessary here to make this valid notebook syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING!\n",
      "Duplicate flags: num_threads\n",
      "Min num runs: [50]\n",
      "Min runs duration (seconds): [1]\n",
      "Max runs duration (seconds): [150]\n",
      "Inter-run delay (seconds): [-1]\n",
      "Num threads: [1]\n",
      "...\n",
      "...\n",
      "Loaded model quicknet.tflite\n",
      "The input model file size (MB): 3.3512\n",
      "Initialized session in 1.471ms.\n",
      "Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\n",
      "count=16 first=41707 curr=31870 min=31722 max=41707 avg=32491.4 std=2395\n",
      "\n",
      "Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\n",
      "count=50 first=31929 curr=31278 min=31138 max=31929 avg=31420.6 std=260\n",
      "\n",
      "Inference timings in us: Init: 1471, First inference: 41707, Warmup (avg): 32491.4, Inference (avg): 31420.6"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/larq/compute-engine/releases/download/v0.4.3/lce_benchmark_model_aarch64 -o lce_benchmark_model\n",
    "!chmod +x lce_benchmark_model\n",
    "!./lce_benchmark_model --graph=quicknet.tflite --num_runs=50 --num_threads=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of interest here is `Inference (avg)`, which in this case is 31.4 ms (31420.6 microseconds) on a [Raspberry Pi 4B](https://www.raspberrypi.org/documentation/hardware/raspberrypi/bcm2711/README.md).\n",
    "\n",
    "To see the other available benchmarking options, add `--help` to the command above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your own Larq model\n",
    "\n",
    "Instead of using one of our models, you probably want to benchmark a custom model that you trained yourself. For more information on creating and training a BNN with Larq, see our [Larq User Guides](/). For best practices on optimizing Larq models for LCE, also see our [Model Optimization Guide](/compute-engine/model_optimization_guide).\n",
    "\n",
    "The code below defines a simple BNN model that takes a 32x32 input image and classifies it into one of 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+sequential_4 stats---------------------------------------------------------------------------------------------+\n",
      "| Layer                       Input prec.           Outputs  # 1-bit  # 32-bit  Memory  1-bit MACs  32-bit MACs |\n",
      "|                                   (bit)                        x 1       x 1    (kB)                          |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| conv2d_5                              -  (-1, 11, 11, 32)        0      2432    9.50           0       290400 |\n",
      "| batch_normalization_29                -  (-1, 11, 11, 32)        0        64    0.25           0            0 |\n",
      "| quant_conv2d_21                       1    (-1, 6, 6, 32)     9216         0    1.12      331776            0 |\n",
      "| batch_normalization_30                -    (-1, 6, 6, 32)        0        64    0.25           0            0 |\n",
      "| activation_10                         -    (-1, 6, 6, 32)        0         0       0           ?            ? |\n",
      "| quant_conv2d_22                       1    (-1, 3, 3, 64)    18432         0    2.25      165888            0 |\n",
      "| batch_normalization_31                -    (-1, 3, 3, 64)        0       128    0.50           0            0 |\n",
      "| activation_11                         -    (-1, 3, 3, 64)        0         0       0           ?            ? |\n",
      "| global_average_pooling2d_4            -          (-1, 64)        0         0       0           ?            ? |\n",
      "| dense_5                               -          (-1, 10)        0       650    2.54           0          640 |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| Total                                                        27648      3338   16.41      497664       291040 |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "+sequential_4 summary--------------------------+\n",
      "| Total params                      31 k       |\n",
      "| Trainable params                  30.7 k     |\n",
      "| Non-trainable params              256        |\n",
      "| Model size                        16.41 KiB  |\n",
      "| Model size (8-bit FP weights)     6.63 KiB   |\n",
      "| Float-32 Equivalent               121.04 KiB |\n",
      "| Compression Ratio of Memory       0.14       |\n",
      "| Number of MACs                    789 k      |\n",
      "| Ratio of MACs that are binarized  0.6310     |\n",
      "+----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a custom model\n",
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input((32, 32, 3), name=\"input\"),\n",
    "        # First layer (float)\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=(5, 5), padding=\"same\", strides=3),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        # Note: we do NOT add a ReLU here, because the subsequent activation quantizer would destroy all information!\n",
    "        # Second layer (binary)\n",
    "        lq.layers.QuantConv2D(\n",
    "            32,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            strides=2,\n",
    "            input_quantizer=\"ste_sign\",\n",
    "            kernel_quantizer=\"ste_sign\",\n",
    "            kernel_constraint=\"weight_clip\",\n",
    "            use_bias=False  # We don't need a bias, since the BatchNorm already has a learnable offset\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation(\"relu\"),\n",
    "        # Third layer (binary)\n",
    "        lq.layers.QuantConv2D(\n",
    "            64,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            strides=2,\n",
    "            input_quantizer=\"ste_sign\",\n",
    "            kernel_quantizer=\"ste_sign\",\n",
    "            kernel_constraint=\"weight_clip\",\n",
    "            use_bias=False  # We don't need a bias, since the BatchNorm already has a learnable offset\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation(\"relu\"),\n",
    "        # Pooling and final dense layer (float)\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "lq.models.summary(model)\n",
    "\n",
    "# Note: Realistically, you would of course want to train your model before converting it!\n",
    "\n",
    "# Convert our Keras model to a TFLite flatbuffer file\n",
    "with open(\"custom_model.tflite\", \"wb\") as flatbuffer_file:\n",
    "    flatbuffer_bytes = lce.convert_keras_model(model)\n",
    "    flatbuffer_file.write(flatbuffer_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is converted, it is useful to visualize it in [Netron](https://lutzroeder.github.io/netron/) to make sure the network looks as expected. Simply go to [Netron](https://lutzroeder.github.io/netron/) and select the `.tflite` file to visualize it (press Ctrl + K to switch to horizontal mode). For the model above, the first part of the flatbuffer looks like this:\n",
    "\n",
    "![](../images/custom_model_netron_1.png)\n",
    "\n",
    "There is an unexpected `Mul` operation between the two binary convolutions, fused with a `ReLU`. Since `BatchNormalization` and `ReLU` can be efficiently fused into the convolution operation, this indicates that something about our model configuration is suboptimal.\n",
    "\n",
    "There are two culprits here, both explained in the [Model Optimization Guide](/compute-engine/model_optimization_guide):\n",
    "\n",
    "1. The order of `BatchNormalization` and `ReLU` is incorrect. Not only does this prevent fusing these operators with the convolution, but since ReLU produces only positive values, the subsequent `LCEQuantize` operation will turn the entire output into ones, and the network cannot learn anything. This can be easily fixed by reversing the order of these two operations:\n",
    "   \n",
    "    ```python\n",
    "    # Example code for a correct ordering of binary convolution, ReLU and BatchNorm.\n",
    "    lq.layers.QuantConv2D(\n",
    "        32,\n",
    "        kernel_size=(3, 3),\n",
    "        padding=\"same\",\n",
    "        strides=2,\n",
    "        input_quantizer=\"ste_sign\",\n",
    "        kernel_quantizer=\"ste_sign\",\n",
    "        kernel_constraint=\"weight_clip\",\n",
    "        use_bias=False   # We don't need a bias, since the BatchNorm already has a learnable offset\n",
    "    )\n",
    "    tf.keras.layers.Activation(\"relu\")\n",
    "    tf.keras.layers.BatchNormalization()\n",
    "    ```\n",
    "\n",
    "2. However, if you change the model definition above to incorporate these changes, the graph looks like this:\n",
    "\n",
    "    ![](../images/custom_model_netron_2.png)\n",
    "\n",
    "    Which is even worse, because there are now two unfused operations (`ReLU` and `Mul`) instead of one (`Mul` with fused `ReLU`).\n",
    "\n",
    "    This is because while the binary convolutions use `padding=\"same\"`, no padding value was specified and therefore the default value of 0 is used. Since binary weights can only take two values, -1 and 1, this 0 cannot also be represented in the existing input tensor, so an additional correction step is necessary and the `ReLU` cannot be fused. This can be resolved by using `pad_values=1` for the binary convolutions:\n",
    "\n",
    "    ```python\n",
    "    # Example code for a fusable configuration of a binary convolution with \"same\" padding, including ReLU and BatchNorm.\n",
    "    lq.layers.QuantConv2D(\n",
    "        32,\n",
    "        kernel_size=(3, 3),\n",
    "        padding=\"same\",\n",
    "        pad_values=1,\n",
    "        strides=2,\n",
    "        input_quantizer=\"ste_sign\",\n",
    "        kernel_quantizer=\"ste_sign\",\n",
    "        kernel_constraint=\"weight_clip\",\n",
    "        use_bias=False   # We don't need a bias, since the BatchNorm already has a learnable offset\n",
    "    )\n",
    "    tf.keras.layers.Activation(\"relu\")\n",
    "    tf.keras.layers.BatchNormalization()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making these final changes to the model definition above, the model looks correct at last:\n",
    "\n",
    "![](../images/custom_model_netron_final.png)\n",
    "\n",
    "The `ReLu` and `BatchNormalization` operations have now successfully been fused into the convolution operation, meaning the inference engine just has to execute a single operation instead of three!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've succesfully created and benchmarked your own BNN, you probably want to use it for a custom application. For information on using the Larq Compute Engine for inference, check out our [C++](/compute-engine/inference) and [Android](/compute-engine/quickstart_android) inference guides."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
# `larq.layers`

Each Quantized Layer requires a `input_quantizer` and `kernel_quantizer` that describes the way of quantizing the activation of the previous layer and the weights respectively.

If both `input_quantizer` and `kernel_quantizer` are `None` the layer is equivalent to a full precision layer.

{{autogenerated}}